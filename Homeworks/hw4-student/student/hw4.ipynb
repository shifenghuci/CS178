{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4494b471",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"hw4.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b7d466",
   "metadata": {
    "id": "43b7d466"
   },
   "source": [
    "# <center> CS 178: Machine Learning &amp; Data Mining </center>\n",
    "## <center> Homework 4: Solutions </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0921f03",
   "metadata": {
    "id": "a0921f03"
   },
   "source": [
    "---\n",
    "## Instructions\n",
    "\n",
    "\n",
    "This homework (and many subsequent ones) will involve data analysis and reporting on methods and results using Python code. **You will submit a zip file containing: [hw4.ipynb, problem1_solution.png]** to Gradescope. Please do not rename the notebook. This includes any text you wish to include to describe your results, the complete code snippets of how you attempted each problem, and any figures that were generated (make sure to run all cells before submitting). It is important that you include enough detail that we know how you solved the problem, since otherwise we will be unable to grade it.\n",
    "\n",
    "Your homework will be given to you as a zipfile containing the data and a Jupyter notebook with problem descriptions and some template code that will help you get started. You **must** use this starter Jupyter notebook to complete your assignment.\n",
    "\n",
    "If you have any questions/concerns about using Jupyter notebooks, ask us on EdD.\n",
    "\n",
    "### Summary of Assignment: 100 total points\n",
    "- Problem 1: Drawing a Decision Tree (15 points)\n",
    "- Problem 2: Computing the Gini Index (15 points)\n",
    "- Problem 3: Implementing Decision Trees (50 points)\n",
    "    - Problem 3.1: `class_prob_vector` (5 points)\n",
    "    - Problem 3.2: `leaf_condition` (10 points)\n",
    "    - Problem 3.3: `gini_score` (10 points)\n",
    "    - Problem 3.4: `find_best_split` (15 points)\n",
    "    - Problem 3.5: `build_tree` (10 points)\n",
    "- Problem 4: Experimenting with Sklearn (15 points)\n",
    "    - Problem 4.1: Training a small DT (5 points)\n",
    "    - Problem 4.2: Varying depth (5 points)\n",
    "    - Problem 4.3: Varying min_leaf (5 points)\n",
    "- Statement of Collaboration (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648cdb14",
   "metadata": {
    "id": "648cdb14"
   },
   "source": [
    "Before we get started, let's import some libraries that you will make use of in this assignment. Make sure that you run the code cell below in order to import these libraries.\n",
    "\n",
    "**Important: In the code block below, we set `seed=1234`. This is to ensure your code has reproducible results and is important for grading. Do not change this. If you are not using the provided Jupyter notebook, make sure to also set the random seed as below.**\n",
    "\n",
    "**Important: Do not change any codes we give you below, except for those waiting for you to complete. This is to ensure your code has reproducible results and is important for grading.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f788ad34",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f788ad34",
    "outputId": "e6f1442e-c54d-4eaf-b2e3-90ca864d2d3f"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_classification, load_breast_cancer\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Fix the random seed for reproducibility\n",
    "# !! Important !! : do not change this\n",
    "seed = 1234\n",
    "np.random.seed(seed)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b35375",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "---\n",
    "## Problem 1 (15 points): Draw a Decision Tree\n",
    "\n",
    "In the image below, you are given some data for a binary classification problem with features $X_1$ and $X_2$ and labels $y = 0$ or $y = 1$. In addition, the dashed black lines depict the splits of a particular decision tree trained on this data.\n",
    "\n",
    "\n",
    "<img src=\"./problem1_tree.png\"  />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e1fef6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "- Draw the decision tree corresponding to the splits in the image. For every leaf node, you should also include $p(y = 1 | \\text{path})$. \n",
    "- Write your answer on paper and include a picture of your answer in this notebook. In order to include an image in Jupyter notebook, save the image in the same directory as the .ipynb file and then write `![caption](problem1_solution.png)`. **The image must be named \"problem1_solution.png\" and be submitted to gradescope with the notebook.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42348cf4",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a54a56",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "---\n",
    "## Problem 2 (15 points): Computing the Gini Index\n",
    "\n",
    "In the table below, you are given some data for a classification problem with 3 features and 3 classes. The feature $X_1$ is a binary feature, and the features $X_2$ and $X_3$ are real-valued features. The three classes are labeled $y = 0, 1, 2$. \n",
    "\n",
    "In this problem, you will use the Gini index to decide which feature to split on. In other words, you will be creating the root of a decision tree based on this data. You should do this by **hand** for practice for your exams, then write out code to solve for the outputs.\n",
    "\n",
    "- What is the Gini index of the data before doing any splitting?\n",
    "- For each feature, compute the Gini index of splitting on that feature. Use a threshold of $t = 0.4$ for $X_2$ and $t = 2.0$ for $X_3$. (Why don't we need to specify a threshold for $X_1$? You don't have to answer this, but you should think about it.) \n",
    "- Based on your answer to the previous question, which feature should we split on for the root of our decision tree?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9f19dc",
   "metadata": {},
   "source": [
    "| X1 | X2   | X3   | y |\n",
    "|----|------|------|---|\n",
    "| 0  | 0.1  | 3.4  | 0 |\n",
    "| 1  | 0.3  | 3.0  | 0 |\n",
    "| 0  | -0.2 | 2.9  | 0 |\n",
    "| 0  | 1.3  | 0.1  | 1 |\n",
    "| 1  | 2.2  | -0.5 | 1 |\n",
    "| 0  | 4.0  | 0.3  | 1 |\n",
    "| 1  | 0.5  | 1.2  | 2 |\n",
    "| 1  | 3.0  | 0.75 | 2 |\n",
    "| 1  | 2.2  | 0.1  | 2 |\n",
    "| 0  | 0.25 | 0.1  | 2 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2c71e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gini_score_before_split = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509574ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gini_score_for_x1 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d6397d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gini_score_for_x2 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31262550",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gini_score_for_x3 = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae6b55f",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a2b0f6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "---\n",
    "## Problem 3: Implementing a Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbef3196",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "You will now implement an algorithm for learning a decision tree from data. You are given some starter code below that you will need to complete. To keep things simple, your implementation will only work for binary classification.\n",
    "\n",
    "The class `Node` represents a single node in a decision tree. This class is already completed for you, and contains several useful attributes. In addition, the class `DecisionTree` is partially implemented for you. Before attempting this problem, it is important that you read and understand both of these classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a62ec46",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\" A class representing a node in a decision tree.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, depth):\n",
    "        self.depth = depth         # What level of the tree this node is at; depth=0 is the root node\n",
    "        \n",
    "        self.split_feature = None  # The index of the feature that this node splits, if any\n",
    "        self.threshold = None      # The threshold used to split the feature\n",
    "        \n",
    "        self.left_child = None     # A node object (or None) representing the left-hand child of this node \n",
    "        self.right_child = None    # A node object (or None) representing the right-hand child of this node\n",
    "        \n",
    "        self.probs = None          # A numpy array of length 2 representing [p(y=0), p(y=1)] at this node\n",
    "        \n",
    "    def __repr__(self):\n",
    "        # Gives a nice looking representation if you call print on a node \n",
    "        if self.is_leaf():\n",
    "            return f'Leaf Node \\n -| Depth: {self.depth} \\n -| Probs {self.probs}'\n",
    "        else:\n",
    "            return f'Internal Node \\n -| Split feature: {self.split_feature} \\n -| Threshold: {self.threshold}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25dfc31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    \"\"\"\n",
    "    A class representing a decision tree.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_depth=3):\n",
    "        self.root = None            # A Node object which is the root of our tree\n",
    "        self.max_depth = max_depth  # An integer representing the maximum allowed depth of the tree\n",
    "        \n",
    "    def class_prob_vector(self, y):\n",
    "        \"\"\"\n",
    "        Given an array of labels y, compute p(y=0) and p(y=1).\n",
    "        \n",
    "        returns: a numpy array containing [p(y=0), p(y=1)].\n",
    "        \"\"\"\n",
    "        ### YOUR CODE STARTS HERE ###\n",
    "        ...\n",
    "        ###  YOUR CODE ENDS HERE  ###\n",
    "        return probs\n",
    "        \n",
    "    def leaf_condition(self, node):\n",
    "        \"\"\"\n",
    "        Given a Node object, returns True if this is a leaf node and False otherwise.\n",
    "        \n",
    "        A Node is considered a leaf node if all labels at the node belong to the same class,\n",
    "            or if the node is at the maximum allowed depth of the tree.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE STARTS HERE ###\n",
    "        ...\n",
    "        ###  YOUR CODE ENDS HERE  ###\n",
    "        \n",
    "    def gini_score(self, X, y, i, threshold):\n",
    "        \"\"\"\n",
    "        Given features X and labels y, computes the Gini index of splitting\n",
    "            the i-th feature at the given threshold.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE STARTS HERE ###\n",
    "        ...\n",
    "        ###  YOUR CODE ENDS HERE  ###\n",
    "        \n",
    "    def find_best_split(self, X, y):\n",
    "        \"\"\"\n",
    "        Given features X and labels y, finds the best split based on the Gini index.\n",
    "        \n",
    "        returns: an index corresponding to which feature we are splitting,\n",
    "            as well as the threshold we are splitting the feature at.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE STARTS HERE ###\n",
    "        ...\n",
    "        ###  YOUR CODE ENDS HERE  ###       \n",
    "        return best_idx, best_threshold\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fits the decision tree given features X and labels y.\n",
    "        \"\"\"\n",
    "        assert isinstance(X, np.ndarray), 'X must be a numpy array'\n",
    "        assert isinstance(y, np.ndarray), 'y must be a numpy array'\n",
    "        \n",
    "        self.build_tree(X, y, 0)\n",
    "    \n",
    "    def build_tree(self, X, y, depth):\n",
    "        \"\"\"\n",
    "        Recursively builds the decision tree.\n",
    "        \"\"\"\n",
    "                \n",
    "        # Create a new node\n",
    "        node = Node(depth)\n",
    "        if depth == 0:\n",
    "            self.root = node\n",
    "            \n",
    "        # Get the class probabilities for this node\n",
    "        node.probs = self.class_prob_vector(y)\n",
    "                \n",
    "        # Check if this new node is a leaf node; otherwise, split it\n",
    "        if self.leaf_condition(node):\n",
    "            return node            \n",
    "        else:            \n",
    "            # Find which feature to split on and the splitting threshold\n",
    "            split_idx, split_threshold = self.find_best_split(X, y)\n",
    "            \n",
    "            # Create left/right splits\n",
    "            left_idx = X[:, split_idx] <= split_threshold\n",
    "            X_L, y_L = ...\n",
    "            X_R, y_R = ...\n",
    "            \n",
    "            # Recursively split the left/right nodes\n",
    "            node_L = ...\n",
    "            node_R = ...\n",
    "            \n",
    "            # Fill in node information\n",
    "            node.split_feature = ...\n",
    "            node.threshold = ...\n",
    "            node.left_child = ...\n",
    "            node.right_child = ...\n",
    "            \n",
    "            return node\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        After fitting the decision tree, this function can be called to make predictions\n",
    "            for every data point in the feature array X. \n",
    "        \"\"\"\n",
    "        y_hat = []\n",
    "        \n",
    "        for x in X:\n",
    "            # Make a prediction for every row in X\n",
    "            y_hat.append(self._predict(x))\n",
    "        \n",
    "        y_hat = np.array(y_hat)\n",
    "        return y_hat\n",
    "                        \n",
    "    def _predict(self, x):\n",
    "        \"\"\"\n",
    "        Makes predictions on individual datapoints x.\n",
    "        \"\"\"\n",
    "        current_node = self.root\n",
    "        \n",
    "        while True:\n",
    "            if self.leaf_condition(current_node):\n",
    "                # If we're at a leaf node, make a prediction based on the probabilities\n",
    "                probs = current_node.probs\n",
    "                y_hat = np.argmax(probs)\n",
    "                return y_hat\n",
    "            else:\n",
    "                # Otherwise, traverse the tree based on the splits\n",
    "                go_left = x[current_node.split_feature] <= current_node.threshold\n",
    "                if go_left:\n",
    "                    current_node = current_node.left_child\n",
    "                else:\n",
    "                    current_node = current_node.right_child\n",
    "                \n",
    "    \n",
    "    def __repr__(self):\n",
    "        # Pretty printing if we call print on our DecisionTree\n",
    "        return f'Decision Tree \\n -| Max Depth: {self.max_depth}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dbe84a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Problem 3.1 class_prob_vector (5 points):\n",
    "\n",
    "- Complete the function `class_prob_vector`. This function takes in array of labels `y` and returns a numpy array containing $p(y=0)$ and $p(y=1)$.\n",
    "- Run the code block given below to test your implementation. If your code is correct, all sanity checks should pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b83f08",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Use this code block to test your implementation in Problem 3.1\n",
    "# Don't change anything here -- just run it\n",
    "\n",
    "dt = DecisionTree(max_depth=3)\n",
    "\n",
    "y = np.array([0, 0, 1, 1, 1])\n",
    "expected = np.array([0.4, 0.6])\n",
    "out = dt.class_prob_vector(y)\n",
    "\n",
    "print(f'Sanity Check 1 passed: {np.array_equal(expected, out)}')\n",
    "\n",
    "y = np.array([1, 1, 1])\n",
    "expected = np.array([0., 1.])\n",
    "out = dt.class_prob_vector(y)\n",
    "\n",
    "print(f'Sanity Check 2 passed: {np.array_equal(expected, out)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5fdb99",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3.1 Class Prob Vector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f315ae",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Problem 3.2 leaf_condition (10 points):\n",
    "\n",
    "In our decision tree implementation, we will consider a node to be a leaf node if either (a) all labels at the node belong to the same class, or (b) the node is at depth `max_depth`, where `max_depth` is an attribute of our `DecisionTree` that we can specify.\n",
    "\n",
    "- Complete the function `leaf_condition`. This function should take in a `Node` object and return True if this node is a leaf node (according to the above criteria) and False otherwise. You should be able to determine this based on the attributes already defined in the `Node` class.\n",
    "- Run the code block given below to test your implementation. If your code is correct, all sanity checks should pass.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e851a645",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Use this code block to test your implementation in Problem 3.2\n",
    "# Don't change anything here -- just run it\n",
    "\n",
    "dt = DecisionTree(max_depth=3)\n",
    "\n",
    "node = Node(depth=2)\n",
    "node.probs = np.array([0.5, 0.5])\n",
    "expected = False\n",
    "out = dt.leaf_condition(node)\n",
    "print(f'Sanity Check 1 passed: {np.array_equal(expected, out)}')\n",
    "\n",
    "node = Node(depth=3)\n",
    "node.probs = np.array([0.5, 0.5])\n",
    "expected = True\n",
    "out = dt.leaf_condition(node)\n",
    "print(f'Sanity Check 2 passed: {np.array_equal(expected, out)}')\n",
    "\n",
    "node = Node(depth=1)\n",
    "node.probs = np.array([1., 0.])\n",
    "expected = True\n",
    "out = dt.leaf_condition(node)\n",
    "print(f'Sanity Check 3 passed: {np.array_equal(expected, out)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42272659",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3.2 Leaf Condition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83828938",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Problem 3.3 gini_score (10 points):\n",
    "\n",
    "Your next task is to implement a function that computes the Gini index.\n",
    "\n",
    "- Complete the function `gini_score`. This function takes in features `X` and labels `y`, as well as a feature index `i` and a scalar `threshold`. Given these inputs, the function `gini_score` should return the Gini index (i.e. a single number) obtained by splitting the `i`th feature in `X` at the specified threshold.\n",
    "- Run the code block given below to test your implementation. If your code is correct, all sanity checks should pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d10700",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Use this code block to test your implementation in Problem 3.3\n",
    "# Don't change anything here -- just run it\n",
    "\n",
    "dt = DecisionTree(max_depth=3)\n",
    "\n",
    "X = np.array([[1,1], [2,8], [4,9], [6,7], [7,4], [8,11], [3,3], [5,5], [9,5], [10,8], [11, 6], [12,10]])\n",
    "y = np.array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1])\n",
    "\n",
    "expected = 5/11\n",
    "out = dt.gini_score(X, y, 0, 1.5)\n",
    "print(f'Sanity Check 1 passed: {np.isclose(expected, out)}')\n",
    "\n",
    "expected = 2/5\n",
    "out = dt.gini_score(X, y, 0, 2.5)\n",
    "print(f'Sanity Check 2 passed: {np.isclose(expected, out)}')\n",
    "\n",
    "expected = 5/11\n",
    "out = dt.gini_score(X, y, 1, 2)\n",
    "print(f'Sanity Check 3 passed: {np.isclose(expected, out)}')\n",
    "\n",
    "expected = 4/9\n",
    "out = dt.gini_score(X, y, 1, 6.5)\n",
    "print(f'Sanity Check 4 passed: {np.isclose(expected, out)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64315857",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3.3 Gini Score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7764c25",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Problem 3.4 find_best_split (15 points): \n",
    "\n",
    "Now, you will use your function `gini_score` to compute the best splits. To do this, you will need to complete the function `find_best_split`. This function takes in features `X` and labels `y`, and returns the feature index and threshold corresponding to the best split as determined by `gini_score`. That is, for every feature and every threshold, you should compute the Gini index of splitting that feature at that threshold, and you should return the index of the feature and the threshold that results in the lowest Gini index.\n",
    "\n",
    "To determine the thresholds, we will use the **midpoint** strategy. That is, given an array of feature values, we will consider all thresholds given by the midpoints between consecutive feature values. Here's an example. Suppose we are given a feature matrix $X$ with four datapoints and two features, given by\n",
    "$$X = \\begin{bmatrix} 1 & 2 \\\\ 1.5 & 2.5 \\\\ 0.75 & -1.0 \\\\ 3.0 & 0.5 \\end{bmatrix}.$$\n",
    "\n",
    "The thresholds to consider when splitting the second feature would then be $[-0.25, 1.25, 2.25]$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Complete the function `find_best_split` as detailed above.\n",
    "- Run the code block given below to test your implementation. If your code is correct, all sanity checks should pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f30a83f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Use this code block to test your implementation in Problem 3.4\n",
    "# Don't change anything here -- just run it\n",
    "\n",
    "dt = DecisionTree(max_depth=3)\n",
    "\n",
    "X = np.array([[1,1], [2,8], [4,9], [6,7], [7,4], [8,11], [3,3], [5,5], [9,5], [10,8], [11, 6], [12,10]])\n",
    "y = np.array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1])\n",
    "\n",
    "expected = (0, 8.5)\n",
    "out = dt.find_best_split(X, y)\n",
    "print(f'Sanity Check 1 passed: {expected == out}')\n",
    "\n",
    "X_L, y_L = X[X[:, 0] <= 8.5], y[X[:, 0] <= 8.5]\n",
    "expected = (1, 6)\n",
    "out = dt.find_best_split(X_L, y_L)\n",
    "print(f'Sanity Check 2 passed: {expected == out}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7536cd3e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3.4 Find Best Split\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e081cd",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Problem 3.5 build_tree (10 points):\n",
    "\n",
    "For the last step in implementing `DecisionTree`, you will need to complete the function `build_tree`. This function uses the functions you implemented in Problems 3.1-3.4 in order to recursively build your decision tree.\n",
    "\n",
    "- Complete the function `build_tree`.\n",
    "- Run the code block given below to test your implementation. If your code is correct, all sanity checks should pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18ecd33",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Use this code block to test your implementation in Problem 3.5\n",
    "# Don't change anything here -- just run it\n",
    "\n",
    "\n",
    "X = np.array([[1,1], [2,8], [4,9], [6,7], [7,4], [8,11], [3,3], [5,5], [9,5], [10,8], [11, 6], [12,10]])\n",
    "y = np.array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1])\n",
    "\n",
    "\n",
    "expected = np.array([0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1])\n",
    "dt = DecisionTree(max_depth=3)\n",
    "dt.fit(X, y)\n",
    "out = dt.predict(X)\n",
    "print(f'Sanity Check passed: {np.allclose(expected, out, atol=1e-3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8978e19e",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 4: Experimenting with Decision Trees\n",
    "\n",
    "In the final problem of this assignment, you will experiment with the scikit-learn implementation of decision trees on the breast cancer wisconsin dataset. This dataset consists of 569 datapoints with 30 real-valued features and 2 possible labels. Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. See the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html) for some more information on this dataset.\n",
    "\n",
    "Before attempting this problem, you should read and understand the documentation for the `DecisionTreeClassifier`, available [here](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html).\n",
    "\n",
    "**Important: for every decision tree classifier you train in this problem, make sure to set `random_state=seed` for reproducibility.**\n",
    "\n",
    "We will first load in this dataset and create a train/test split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482aa905",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.3, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a18ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7238f038",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_te.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8dc7ac",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Problem 4.1 Training a small DT (5 points):\n",
    "\n",
    "First, you will see how to fit and visualize a decision tree classifier in sklearn.\n",
    "\n",
    "- Using the class `DecisionTreeClassifier` from scikit-learn, train a decision tree on the training data. Use `max_depth=2`, and leave all other settings as their defaults. Note that, by default, `DecisionTreeClassifier` uses the Gini index to split nodes.\n",
    "- Calculate the training set error and testing set error of your classifier.\n",
    "- Use the function `plot_tree` to visualize your decision tree. This is already imported for you at the top of this notebook. See [here](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html) for the corresponding documentation page.\n",
    "- For the first split of the data, what feature and what threshold is being used in your classifier? You don't need to find the name of this feature, just its index, i.e. which column of `X`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ee5dd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### YOUR CODE STARTS HERE ###\n",
    "clf = ...\n",
    "...\n",
    "\n",
    "clf_err_tr = ...\n",
    "clf_err_te = ...\n",
    "\n",
    "print(clf_err_tr)\n",
    "print(clf_err_te)\n",
    "\n",
    "...\n",
    "###  YOUR CODE ENDS HERE  ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db302c38",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001fac32",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Problem 4.2 Varying depth (5 points):\n",
    "\n",
    "You will now vary the maximum depth allowed in your decision tree and see what effect this has on the error rate.\n",
    "\n",
    "- Train a decision tree for every value of `max_depth` in `[1, 2, ..., 15]`. Use the default settings (other than `max_depth`). \n",
    "- Plot the resulting training and testing set accuracies as a function of depth. Be sure to include an x-label, a y-label, and a legend in your plot.\n",
    "- Describe what you see happen as you increase the depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20f8ce0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### YOUR CODE STARTS HERE ###\n",
    "...\n",
    "###  YOUR CODE ENDS HERE  ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead885df",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a15368",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Problem 4.3 Varying min_leaf (5 points):\n",
    "\n",
    "Lastly, you will vary the minimum number of datapoints allowed in a leaf node in your decision tree and see what effect this has on the error rate.\n",
    "\n",
    "- Train a decision tree with `citerion='gini'` for every value of `min_leaf_samples` in `[1, 2, ..., 15]`. \n",
    "- Plot the resulting training and testing set accuracies as a function of the minimum leaf samples. Be sure to include an x-label, a y-label, and a legend in your plot.\n",
    "- You should see that the training error increases as we increase the minimum number of leaf samples. Give an explanation for why this might happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a6dc12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### YOUR CODE STARTS HERE ###\n",
    "...\n",
    "###  YOUR CODE ENDS HERE  ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712c3d26",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a052765",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "4a052765"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "---\n",
    "### Statement of Collaboration (5 points)\n",
    "\n",
    "It is **mandatory** to include a Statement of Collaboration in each submission, with respect to the guidelines below. Include the names of everyone involved in the discussions (especially in-person ones), and what was discussed. If you did not collaborate with anyone, you should write something like \"I completed this assignment without any collaboration.\"\n",
    "\n",
    "All students are required to follow the academic honesty guidelines posted on the course website. For\n",
    "programming assignments, in particular, I encourage the students to organize (perhaps using EdD) to\n",
    "discuss the task descriptions, requirements, bugs in my code, and the relevant technical content before they start\n",
    "working on it. However, you should not discuss the specific solutions, and, as a guiding principle, you are not\n",
    "allowed to take anything written or drawn away from these discussions (i.e. no photographs of the blackboard,\n",
    "written notes, referring to EdD, etc.). Especially after you have started working on the assignment, try\n",
    "to restrict the discussion to EdD as much as possible, so that there is no doubt as to the extent of your\n",
    "collaboration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac3ce3d",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1376982",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "otter": {
   "OK_FORMAT": false,
   "assignment_name": "hw4",
   "tests": {
    "q2.1 Gini Score Before Splitting": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"q2.1 Gini Score Before Splitting\"\npoints = 3\n\n",
    "q2.2 Gini Score for X1": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"q2.2 Gini Score for X1\"\npoints = 3\n\n",
    "q2.3 Gini Score for X2": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"q2.3 Gini Score for X2\"\npoints = 3\n\n",
    "q2.4 Gini Score for X3": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"q2.4 Gini Score for X3\"\npoints = 3\n\n",
    "q3.1 Class Prob Vector": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"q3.1 Class Prob Vector\"\npoints = 5\n\n@test_case(points=5, hidden=False)\ndef test_class_prob_vector(DecisionTree, np):\n    dt_31 = DecisionTree(max_depth=3)\n    y_31 = np.array([0, 0, 0, 0, 1])\n    expected_31 = np.array([0.8, 0.2])\n    out_31 = dt_31.class_prob_vector(y_31)\n\n    assert np.allclose(expected_31, out_31, atol=1e-3)\n\n",
    "q3.2 Leaf Condition": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"q3.2 Leaf Condition\"\npoints = 10\n\n@test_case(points=10, hidden=False)\ndef test_leaf_condition(Node, DecisionTree, np):\n    dt_32a = DecisionTree(max_depth=4)\n    node_32a = Node(depth=3)\n    node_32a.probs = np.array([1, 0])\n    expected_32a = True\n    out_32a = dt_32a.leaf_condition(node_32a)\n\n    dt_32b = DecisionTree(max_depth=4)\n    node_32b = Node(depth=4)\n    node_32b.probs = np.array([0.5, 0.5])\n    expected_32b = True\n    out_32b = dt_32b.leaf_condition(node_32b)\n\n    assert np.allclose(expected_32a, out_32a, atol=1e-3) and np.allclose(expected_32b, out_32b, atol=1e-3)\n\n",
    "q3.3 Gini Score": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"q3.3 Gini Score\"\npoints = 10\n\n@test_case(points=10, hidden=False)\ndef test_gini_score(DecisionTree, np):\n    dt_33 = DecisionTree(max_depth=3)\n    X_33 = np.array([[1,1], [2,8], [4,9], [6,7], [7,4], [8,11], [3,3], [5,5], [9,5], [10,8], [11, 6], [12,10]])\n    y_33 = np.array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1])\n    \n    expected_33 = 1/4\n    out_33 = dt_33.gini_score(X_33, y_33, 0, 8)\n\n    assert np.isclose(expected_33, out_33, atol=1e-3)\n\n",
    "q3.4 Find Best Split": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"q3.4 Find Best Split\"\npoints = 15\n\n@test_case(points=15, hidden=False)\ndef test_find_best_split(DecisionTree, np):\n    dt_34 = DecisionTree(max_depth=3)\n    X_34 = np.array([[1,1], [2,8], [4,9], [6,7], [7,4], [8,11], [3,3], [5,5], [9,5], [10,8], [11, 6], [12,10]])\n    y_34 = np.array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1])\n    \n    X_L_34, y_L_34 = X_34[X_34[:, 0] <= 8.5], y_34[X_34[:, 0] <= 8.5]\n    expected_34 = (1, 6)\n    out_34 = dt_34.find_best_split(X_L_34, y_L_34)\n\n    assert np.allclose(expected_34, out_34, atol=1e-3)\n\n",
    "q3.5 Build Tree": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"q3.5 Build Tree\"\npoints = 10\n\n",
    "q4.1b Train and Test Error": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"q4.1b Train and Test Error\"\npoints = 1\n\n"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
